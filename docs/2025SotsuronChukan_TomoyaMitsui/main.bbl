% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold \emph{et~al.}, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \emph{International Conference on Learning Representations (ICLR)}, 2021, pp. 1--22.

\bibitem{wang2024comprehensive}
L.~Wang, X.~Zhang, H.~Su, and J.~Zhu, ``A comprehensive survey of continual learning: Theory, method and application,'' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, vol.~46, no.~8, pp. 5362--5383, 2024.

\bibitem{parisi2019continual}
G.~I. Parisi, R.~Kemker, J.~L. Part, C.~Kanan, and S.~Wermter, ``Continual lifelong learning with neural networks: A review,'' \emph{Neural networks}, vol. 113, pp. 54--71, 2019.

\bibitem{rebuffi2017icarl}
S.-A. Rebuffi, A.~Kolesnikov, G.~Sperl, and C.~H. Lampert, ``icarl: Incremental classifier and representation learning,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2017, pp. 2001--2010.

\bibitem{mundt2023wholistic}
M.~Mundt, Y.~Hong, I.~Pliushch, and V.~Ramesh, ``A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning,'' \emph{Neural Networks}, vol. 160, pp. 306--336, 2023.

\bibitem{french1999catastrophic}
R.~M. French, ``Catastrophic forgetting in connectionist networks,'' \emph{Trends in cognitive sciences}, vol.~3, no.~4, pp. 128--135, 1999.

\bibitem{kirkpatrick2017overcoming}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A. Rusu, K.~Milan, J.~Quan, T.~Ramalho \emph{et~al.}, ``Overcoming catastrophic forgetting in neural networks,'' \emph{Proceedings of the national academy of sciences}, vol. 114, no.~13, pp. 3521--3526, 2017.

\bibitem{mermillod2013stability}
M.~Mermillod, A.~Bugaiska, and P.~Bonin, ``The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects,'' p. 504, 2013.

\bibitem{houlsby2019parameter}
N.~Houlsby, A.~Giurgiu, S.~Jastrzebski, B.~Morrone, Q.~De~Laroussilhe, A.~Gesmundo, M.~Attariyan, and S.~Gelly, ``Parameter-efficient transfer learning for nlp,'' in \emph{International conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp. 2790--2799.

\bibitem{hu2022lora}
E.~J. Hu, yelong shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen, ``Lo{RA}: Low-rank adaptation of large language models,'' in \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{yu2024boosting}
J.~Yu, Y.~Zhuge, L.~Zhang, P.~Hu, D.~Wang, H.~Lu, and Y.~He, ``Boosting continual learning of vision-language models via mixture-of-experts adapters,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 23\,219--23\,230.

\bibitem{wang2025self}
H.~Wang, H.~Lu, L.~Yao, and D.~Gong, ``Self-expansion of pre-trained models with mixture of adapters for continual learning,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2025, pp. 10\,087--10\,098.

\bibitem{mcclelland1995there}
J.~L. McClelland, B.~L. McNaughton, and R.~C. O'Reilly, ``Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.'' \emph{Psychological review}, vol. 102, no.~3, p. 419, 1995.

\bibitem{kumaran2016learning}
D.~Kumaran, D.~Hassabis, and J.~L. McClelland, ``What learning systems do intelligent agents need? complementary learning systems theory updated,'' \emph{Trends in cognitive sciences}, vol.~20, no.~7, pp. 512--534, 2016.

\bibitem{behrouz2025nested}
A.~Behrouz, M.~Razaviyayn, P.~Zhong, and V.~Mirrokni, ``Nested learning: The illusion of deep learning architectures,'' in \emph{The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2025.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton \emph{et~al.}, ``Learning multiple layers of features from tiny images,'' 2009.

\bibitem{sokar2025continual}
G.~Sokar, G.~K. Dziugaite, A.~Arnab, A.~Iscen, P.~S. Castro, and C.~Schmid, ``Continual learning in vision-language models via aligned model merging,'' \emph{arXiv preprint arXiv:2506.03189}, 2025.

\end{thebibliography}
