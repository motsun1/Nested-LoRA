## 1. いま考えている「手法の地図」

### 軸1：構造（LoRAの持ち方）

1. **Chain型（HOPE寄り）**

   * fast → slow（→ medium…）みたいに**1系列だけ**の LoRA。
   * タスクごとに「そのタスク用 fast」を学習し、終わったら slow に統合。
   * 最終的に使うのは slow（＋必要なら fast の一部）。

2. **MoE型**

   * fast をたくさん用意（LoRAバンク）。
   * 各タスクで

     * どれか1つを選んで学習する、
     * あるいは複数に重みを付けて学習。
   * slow への統合も「どの fast をどれだけ反映するか」を重み付きでやる。

---

### 軸2：fast→slow の知識移行（consolidation）

1. **Task Arithmetic（いま）**

   * `slow += α * fast` みたいな素朴な足し算。
   * 破滅的忘却は**ほぼ抑えられていない**ことが現実に分かった。

2. **Model Merge 系**

   * **PAM**：学習中に「global と符号がズレる重み」を抑制して、あとで足し算しやすくする。
   * **TIES**：trim（小さいデルタを0）＋符号・大きさでマージ。
   * **TSV/SVD**：低ランク方向だけ抽出してマージ。
   * 要するに「**足し算の仕方を賢くする**」系。

3. **HOPE みたいな「ちゃんと入れ子構造」**

   * 明示的に複数タイムスケール（短期/中期/長期）を用意して、
   * それぞれの更新頻度・役割を設計して入れ子にする。
   * 単なるマージじゃなくて**“階層記憶”としての設計**をちゃんとやる路線。

---

## 2. いまの状況から見えること

* 今の **Chain × Task Arithmetic**（fast→slow を `slow += 0.1*fast`）だと、

  * LoRA あり（slow-only, baseline）は **fast-only より総合精度が低い or 同等**、
  * 忘却もほとんど抑えられていない。
* つまり、

  * 「Nested にしたから忘却が減りました」とは全く言えない状態。
  * **consolidation ルールが弱い／線形層が暴れすぎ**のどちらか（おそらく両方）を疑うべきフェーズ。

ここを踏まえると、

> MoE を試すのは全然アリだけど、
> **“弱い基盤（Task Arithmetic + ガチ忘却）”の上に MoE を載せても、
> 分析がぐちゃるリスクが高い**

というのも事実。

---

## 3. 「線形層がSEMA仕様になってないか」のチェック

これは一回ちゃんと見た方がいい。

やりたいこと（君の問題設定）って：

* **「クラスインクリメンタル学習 + 1つの共通線形層」**で、
* そこに **Nested LoRA / モデルマージ**の工夫を効かせたい、だよね。

一方 SEMA側のコードは、

* タスクごとにアダプタやヘッドを増やすロジックや、
* MoE ルータを持っていたりする。

なので、

* **線形層がタスクごとに分かれていないか**
* **推論時に本当に「単一ヘッド + 全タスククラス」に対して評価しているか**
* **SEMAの expansion 用の分岐が残ってないか**

ここは **コードレベルで1回だけガッと確認しておく価値はデカい**。
もしここがズレてると、その上にどんな Nested を載せても論文の問題設定がブレるので。

---

## 4. 具体的に「次にやると良さそうな順番」

君の「MoEも早く一回動かしてみたい」気持ちも分かるので、
**実装コストとリスクのバランスを取るプラン**を出すね。

### Step 0：線形層の仕様チェック（優先度かなり高）

* 確認したいこと：

  1. classifier が「タスク共有の 1本」であること
     （タスクIDでヘッド切り替えとかしてない）
  2. inference 時も **常にその1本の classifier だけ**使っていること
  3. SEMA の `expand_classifier()` 的な処理が無効化されているか

ここはコード読むだけ／print するだけで済むので、
**MoEに行く前の「サニティチェック」として1回やっておくのがおすすめ。**

---

### Step 1：Chain構造のまま「まともな fast→slow マージ」を1つ入れる

今の `slow += 0.1 * fast` が弱いのはハッキリしたので、

* **Chain × Task Arithmetic (現状)**
* vs
* **Chain × “マシな Model Merge”（TIES-lite or PAM-lite）**

をまず比較しておくと、

* 「Nested っぽい構造＋マージを真面目にやると、fast-only よりちゃんと意味があるのか」
* 「単に lr 下げるだけと比べてどうか」

が見える。

おすすめは、**PAM-lite” を1個入れること**：

> ## ✨ 3. PAM（Pre-Alignment Merge）とは（CL には最適）
> 
> 論文: *Continual Learning in Vision-Language Models via Aligned Model Merging*（Sokar et al.）
> 
> Sokar は VLM 用だけど、原理は画像分類にも普通に通用する。
>
> ### ✔ PAM の本質：
>
>> **「マージをするときに壊れるのは、そもそも LoRA が slow と反対方向に勾配を打ってるから」
>> → なら、“学習中に” slow との整合性を強制して、fast に変な方向の成分が生まれないように調整する。**
>
>つまり、
>
> * TIES/TIES-lite = **「マージ時に壊れるのを後で修正する」**
> * PAM = **「壊れる更新が生まれないように “事前に” alignment しておく」**
>
>という決定的な違いがある。
>
>### ✔ PAM がやること：
>
>1. **slow と fast の符号が逆向きの位置**を検出
>2. **fast のその要素を正しい方向に re-align（再初期化 or 抑制）**
>3. その状態で fast を学習するので、
>   **fast は “slow と合成しやすい” LoRA に育つ**
>
>こうすることで、
>
>* 最後に単純な `slow += α fast` をしても壊れにくくなる
>* モデルマージが自然に機能する
>
>### ✔ CL に最適な理由：
>
>* **“継続学習”はタスクごとに fast を作り続ける**
>  → fast の方向が slow と合わなくなるのが破滅的忘却の原因
>
>* PAM はその部分を根本的に抑える
>  → **fast が slow と同じ座標系に揃う → マージしても壊れない**
>
>* slow は「整合したタスク方向の累積」になる
>  → **ゆっくり成長する長期記憶になる**
>
>### ✔ まとめ：
>
>**継続学習の slow LoRA を安定化させるなら
>TIES/TIES-lite より PAM の方が目的に合っている。**


「Chain構造のまま、Task Arithmetic → PAM-lite に変えたら忘却が改善するか？」

を見ておくと、**後で MoE と組み合わせたときのインタプリもしやすい**。

---

### Step 2：MoE（多 fast）を「今のマージ or TIES-lite」で一回動かす

君がやりたい MoE 路線はここ。

* 構造：

  * fast をタスク数 or それ以上だけ用意（LoRAバンク）
  * タスク t では fast_t だけ更新（あるいは複数に重み）
  * slow は全タスク共通
* consolidation：

  * まずはシンプルに「fast_t だけを slow とマージ」
  * ここで使うマージルールを

    * ① 現行の Task Arithmetic
    * ② Step1 で入れた PAM-lite
  * の2パターンで比較できると理想。

これで、

* **Chain vs MoE**
* **Task Arithmetic vs PAM-lite**

の 2×2 が一気に見えて、「どの軸が効いてそうか」がかなりクリアになる。

---

### Step 3：HOPE的な「ちゃんと入れ子構造」に進むかどうか決める

ここまでの結果を踏まえて、

* もし

  * 「Chain × PAM-lite」でそこそこ忘却が抑制される
  * あるいは「MoE × PAM-lite」でいい感じ
  * のであれば、
* その上で

  * fast / mid / slow の **3タイムスケール**
  * 更新頻度やマージの設計
  * を HOPE っぽく整理していく

という流れが自然かなと。

**この時点ではもう「どの構造・どのマージが良かったか」の実験結果が揃ってるはずなので、HOPE 的な設計にも根拠を持って踏み込める。**

---

## 5. まとめ（超ざっくり）

* 大枠の2軸整理はその理解でOK 👍
* ただし今の `slow += 0.1 * fast` は**ほぼ効いてない**ので、

  * **consolidation 方式を1個マシにする（PAM-lite）**
  * ＋
  * **線形層の仕様（SEMA 残骸がないか）を確認**
    を先にやると、後の MoE の結果が読みやすくなる。
* そのうえで、**MoE 版（多 fast）を一回動かすのは全然アリ**。
  ただし、

  * 「Chain vs MoE」
  * 「Task Arithmetic vs 改良マージ」
    を揃えて比べられるように、小さい改良を先に1個入れておくと良い、という感じ。

もしよければ次で、
**「Chain × TIES-lite の fast→slow マージ関数」の擬似コード**を、
そのままコピペ実装できるレベルで書き起こすので、そこで一気に土台固めちゃおう。
