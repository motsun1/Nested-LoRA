ログ内容の要約と簡易分析です。
実行コマンド: conda run -n sema_env python main.py --config exps/nested_lora_cifar.json
ログ: logs/sema/cifar224/0/10/nested_lora_1993_pretrained_vit_b16_224_adapter.log

設定: CIFAR-100 10タスク (init=10, inc=10)、Nested LoRA rank=16、fast lr=0.01 / slow lr=0.001、slow更新間隔=5、consolidationオフ。func 5epoch + rd 20epoch/タスク。
モデル規模: 総パラメータ約87M、学習対象パラメータは初回 1.21M、その後は追加アダプタのみで約76.9K。
タスクごとの結果 (Test Acc / top1):
T0: 98.50
T1: 95.80 (平均 97.15)
T2: 94.30 (平均 96.20)
T3: 93.25 (平均 95.46)
T4: 91.26 (平均 94.62)
T5: 90.10 (平均 93.87)
T6: 89.77 (平均 93.28)
T7: 87.54 (平均 92.57)
T8: 86.82 (平均 91.93)
T9: 86.28 (平均 91.36)
セグメント別 (最終時点):
00-09: 84.8, 10-19: 83.2, 20-29: 92.8, 30-39: 90.4, 40-49: 83.3, 50-59: 78.6, 60-69: 87.1, 70-79: 83.0, 80-89: 88.4, 90-99: 91.2
動的拡張: 各タスクごとに block11 に新アダプタ追加（最後で block10 にも 2 回追加）。optimizer の param group は “other” のみで lr=0.005（Nested LoRA用の fast/slow 分離ではなく fallback パス）。※ログ上 fast/slow グループログが出ていないため、この実行では ffn_adapter_type 判定で Nested LoRA 分岐に入っていない可能性があります（要確認）。
forgetting: 初期 98.5 → 最終 86.28 まで徐々に低下。旧クラス（old）は 85.73、新クラス（new）は 91.2。
懸念/次アクション案

optimizer 分岐を確認：ログが [SEMA] optimizer params=12, lr=0.005 のままなので、Nested LoRA の fast/slow param groupが無効化されている疑い。ffn_adapter_type が渡っているか、update_optimizer_and_scheduler の条件を確認。
consolidation オン試行：nested_lora_use_consolidation: true, alpha を {0.05,0.1} で比較。
slow 更新間隔の短縮：nested_lora_update_interval_slow=1 で安定性と forgetting を比較。
rank/学習率アブレーション：rank=8、lr_fast/lr_slow 比率=5x など。
ベースライン比較：ffn_adapter_type=adaptmlp 同パラメータ規模で 10タスクを走らせ、平均精度/forgetting を比較。
ログ確認コマンド例

tail -n 50 logs/sema/cifar224/0/10/nested_lora_1993_pretrained_vit_b16_224_adapter.log
grep "func Task" logs/sema/cifar224/0/10/nested_lora_1993_pretrained_vit_b16_224_adapter.log
この結果を踏まえて、まずは (1) Nested LoRA の param group が効いているか確認→有効化のうえで (2)(3) の設定を回すのがおすすめです。